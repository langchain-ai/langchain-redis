{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis Cache for LangChain\n",
    "\n",
    "This notebook demonstrates how to use the `RedisCache` and `RedisSemanticCache` classes from the langchain-redis package to implement caching for LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and ensure we have a Redis instance running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /home/jupyter/venv/lib/python3.11/site-packages (0.2.11)\n",
      "Requirement already satisfied: langchain-redis in /home/jupyter/venv/lib/python3.11/site-packages (0.0.1)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: redis in /home/jupyter/venv/lib/python3.11/site-packages (5.0.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (0.1.83)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (2.8.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-core) (8.4.2)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-redis) (1.26.4)\n",
      "Requirement already satisfied: python-ulid<3.0.0,>=2.7.0 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-redis) (2.7.0)\n",
      "Requirement already satisfied: redisvl<0.3.0,>=0.2.3 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-redis) (0.2.3)\n",
      "Requirement already satisfied: sentence-transformers<4.0.0,>=3.0.1 in /home/jupyter/venv/lib/python3.11/site-packages (from langchain-redis) (3.0.1)\n",
      "Collecting openai<2.0.0,>=1.32.0 (from langchain-openai)\n",
      "  Downloading openai-1.35.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jupyter/venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jupyter/venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (3.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/jupyter/venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jupyter/venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.32.0->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jupyter/venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/jupyter/venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/jupyter/venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/jupyter/venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /home/jupyter/venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (2.20.0)\n",
      "Requirement already satisfied: coloredlogs in /home/jupyter/venv/lib/python3.11/site-packages (from redisvl<0.3.0,>=0.2.3->langchain-redis) (15.0.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.9.0 in /home/jupyter/venv/lib/python3.11/site-packages (from redisvl<0.3.0,>=0.2.3->langchain-redis) (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /home/jupyter/venv/lib/python3.11/site-packages (from sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/jupyter/venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jupyter/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/jupyter/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jupyter/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/jupyter/venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyter/venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (2.2.2)\n",
      "Requirement already satisfied: sympy in /home/jupyter/venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/jupyter/venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/jupyter/venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (3.1.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jupyter/venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (0.19.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/jupyter/venv/lib/python3.11/site-packages (from coloredlogs->redisvl<0.3.0,>=0.2.3->langchain-redis) (10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jupyter/venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jupyter/venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jupyter/venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/jupyter/venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers<4.0.0,>=3.0.1->langchain-redis) (1.3.0)\n",
      "Downloading langchain_openai-0.1.14-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.35.9-py3-none-any.whl (328 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 langchain-openai-0.1.14 openai-1.35.9 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-core langchain-redis langchain-openai redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have a Redis server running. You can start one using Docker with:\n",
    "\n",
    "```\n",
    "docker run -d -p 6379:6379 redis:latest\n",
    "```\n",
    "\n",
    "Or install and run Redis locally according to your operating system's instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://redis:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_redis import RedisCache, RedisSemanticCache\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.schema import Generation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core\n",
    "import langchain_openai\n",
    "import openai\n",
    "import redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Check if OPENAI_API_KEY is already set in the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API key not found in environment variables.\")\n",
    "    openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "    # Set the API key for the current session\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"OpenAI API key has been set for this session.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RedisCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (not cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 1.16 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 0.05 seconds\n",
      "\n",
      "Speed improvement: 25.40x faster\n",
      "Cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "# Function to measure execution time\n",
    "def timed_completion(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# First call (not cached)\n",
    "prompt = \"Explain the concept of caching in three sentences.\"\n",
    "result1, time1 = timed_completion(prompt)\n",
    "print(f\"First call (not cached):\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = timed_completion(prompt)\n",
    "print(f\"Second call (cached):\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the cache\n",
    "redis_cache.clear()\n",
    "print(\"Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query:\n",
      "Prompt: What is the capital of France?\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 1.52 seconds\n",
      "\n",
      "Similar query:\n",
      "Prompt: Can you tell me the capital city of France?\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 0.29 seconds\n",
      "\n",
      "Speed improvement: 5.22x faster\n",
      "Semantic cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisSemanticCache\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2\n",
    ")\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "\n",
    "# Function to test semantic cache\n",
    "def test_semantic_cache(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# Original query\n",
    "original_prompt = \"What is the capital of France?\"\n",
    "result1, time1 = test_semantic_cache(original_prompt)\n",
    "print(\n",
    "    f\"Original query:\\nPrompt: {original_prompt}\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\"\n",
    ")\n",
    "\n",
    "# Semantically similar query\n",
    "similar_prompt = \"Can you tell me the capital city of France?\"\n",
    "result2, time2 = test_semantic_cache(similar_prompt)\n",
    "print(\n",
    "    f\"Similar query:\\nPrompt: {similar_prompt}\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the semantic cache\n",
    "semantic_cache.clear()\n",
    "print(\"Semantic cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom TTL (Time-To-Live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached result: Cached response\n",
      "Waiting for TTL to expire...\n",
      "Result after TTL: Not found (expired)\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisCache with custom TTL\n",
    "ttl_cache = RedisCache(redis_url=REDIS_URL, ttl=5)  # 60 seconds TTL\n",
    "\n",
    "# Update a cache entry\n",
    "ttl_cache.update(\"test_prompt\", \"test_llm\", [Generation(text=\"Cached response\")])\n",
    "\n",
    "# Retrieve the cached entry\n",
    "cached_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "print(f\"Cached result: {cached_result[0].text if cached_result else 'Not found'}\")\n",
    "\n",
    "# Wait for TTL to expire\n",
    "print(\"Waiting for TTL to expire...\")\n",
    "time.sleep(6)\n",
    "\n",
    "# Try to retrieve the expired entry\n",
    "expired_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "print(\n",
    "    f\"Result after TTL: {expired_result[0].text if expired_result else 'Not found (expired)'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original result: \n",
      "\n",
      "The largest planet in our solar system is Jupiter.\n",
      "Similar query result: \n",
      "\n",
      "The largest planet in our solar system is Jupiter.\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisSemanticCache with custom settings\n",
    "custom_semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL,\n",
    "    embeddings=embeddings,\n",
    "    distance_threshold=0.1,  # Stricter similarity threshold\n",
    "    ttl=3600,  # 1 hour TTL\n",
    "    name=\"custom_cache\",  # Custom cache name\n",
    ")\n",
    "\n",
    "# Test the custom semantic cache\n",
    "set_llm_cache(custom_semantic_cache)\n",
    "\n",
    "test_prompt = \"What's the largest planet in our solar system?\"\n",
    "result, _ = test_semantic_cache(test_prompt)\n",
    "print(f\"Original result: {result}\")\n",
    "\n",
    "# Try a slightly different query\n",
    "similar_test_prompt = \"Which planet is the biggest in the solar system?\"\n",
    "similar_result, _ = test_semantic_cache(similar_test_prompt)\n",
    "print(f\"Similar query result: {similar_result}\")\n",
    "\n",
    "# Clean up\n",
    "custom_semantic_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the usage of `RedisCache` and `RedisSemanticCache` from the langchain-redis package. These caching mechanisms can significantly improve the performance of LLM-based applications by reducing redundant API calls and leveraging semantic similarity for intelligent caching. The Redis-based implementation provides a fast, scalable, and flexible solution for caching in distributed systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
