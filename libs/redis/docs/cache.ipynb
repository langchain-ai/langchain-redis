{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Redis Cache for LangChain\n",
    "\n",
    "This notebook demonstrates how to use the `RedisCache`, `RedisSemanticCache`, and `LangCacheSemanticCache` classes from the langchain-redis package to implement caching for LLM responses."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and ensure we have a Redis instance running."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:41.800439Z",
     "start_time": "2025-11-15T01:55:38.869222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install -qU langchain-core langchain-redis \"langchain-openai>=1.0.3\" \"redis<7.0\"\n",
    "%pip install -qU -e \"../[langcache]\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ensure you have a Redis server running. You can start one using Docker with:\n",
    "\n",
    "```\n",
    "docker run -d -p 6379:6379 redis:latest\n",
    "```\n",
    "\n",
    "Or install and run Redis locally according to your operating system's instructions."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:41.806781Z",
     "start_time": "2025-11-15T01:55:41.804858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ruff: noqa: T201\n",
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Required Libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:43.782562Z",
     "start_time": "2025-11-15T01:55:41.813175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.outputs import Generation\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_redis import RedisCache, RedisSemanticCache, LangCacheSemanticCache"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.brookins/src/langchain-redis/libs/redis/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set OpenAI API key"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:43.825483Z",
     "start_time": "2025-11-15T01:55:43.823677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if OPENAI_API_KEY is already set in the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API key not found in environment variables.\")\n",
    "    openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "    # Set the API key for the current session\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"OpenAI API key has been set for this session.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found in environment variables.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using RedisCache"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:44.767366Z",
     "start_time": "2025-11-15T01:55:43.829967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "# Function to measure execution time\n",
    "def timed_completion(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# First call (not cached)\n",
    "prompt = \"Explain the concept of caching in three sentences.\"\n",
    "result1, time1 = timed_completion(prompt)\n",
    "print(f\"First call (not cached):\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = timed_completion(prompt)\n",
    "print(f\"Second call (cached):\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the cache\n",
    "redis_cache.clear()\n",
    "print(\"Cache cleared\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:55:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "First call (not cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 0.69 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 0.00 seconds\n",
      "\n",
      "Speed improvement: 483.60x faster\n",
      "Cache cleared\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using RedisSemanticCache"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:46.520639Z",
     "start_time": "2025-11-15T01:55:44.787172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize RedisSemanticCache\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2\n",
    ")\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "\n",
    "# Function to test semantic cache\n",
    "def test_semantic_cache(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# Original query\n",
    "original_prompt = \"What is the capital of France?\"\n",
    "result1, time1 = test_semantic_cache(original_prompt)\n",
    "print(f\"Original query:\\nPrompt: {original_prompt}\\n\")\n",
    "print(f\"Result: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Semantically similar query\n",
    "similar_prompt = \"Can you tell me the capital city of France?\"\n",
    "result2, time2 = test_semantic_cache(similar_prompt)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt}\\n\")\n",
    "print(f\"Result: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the semantic cache\n",
    "semantic_cache.clear()\n",
    "print(\"Semantic cache cleared\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:55:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "17:55:45 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "17:55:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "17:55:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Original query:\n",
      "Prompt: What is the capital of France?\n",
      "\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 1.18 seconds\n",
      "\n",
      "17:55:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Similar query:\n",
      "Prompt: Can you tell me the capital city of France?\n",
      "\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 0.23 seconds\n",
      "\n",
      "Speed improvement: 5.15x faster\n",
      "Semantic cache cleared\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using LangCacheSemanticCache"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Redis LangCache is a managed service that provides a semantic cache for LLM applications. It manages embeddings and vector search for you, allowing you to focus on your application logic. See [our docs](https://redis.io/docs/latest/develop/ai/langcache/) to learn more.\n",
    "\n",
    "**NOTE:** To run these LangCache examples, you must first create a LangCache instance in Redis Cloud. [Get started with a free Redis Cloud account today](https://redis.io/docs/latest/operate/rc/langcache/#get-started-with-langcache-on-redis-cloud)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T01:55:48.215662Z",
     "start_time": "2025-11-15T01:55:46.538770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if LangCache API key and cache ID are already set in the environment\n",
    "langcache_api_key = os.getenv(\"LANGCACHE_API_KEY\")\n",
    "langcache_cache_id = os.getenv(\"LANGCACHE_CACHE_ID\")\n",
    "\n",
    "\n",
    "if not langcache_api_key or not langcache_cache_id:\n",
    "    print(\"LangCache API key or cache ID not found in environment variables.\")\n",
    "    if not langcache_api_key:\n",
    "        langcache_api_key = getpass(\"Please enter your LangCache API key: \")\n",
    "    if not langcache_cache_id:\n",
    "        langcache_cache_id = input(\"Please enter your LangCache cache ID: \")\n",
    "\n",
    "    # Set the API key for the current session\n",
    "    os.environ[\"LANGCACHE_API_KEY\"] = langcache_api_key\n",
    "    os.environ[\"LANGCACHE_CACHE_ID\"] = langcache_cache_id\n",
    "    print(\"LangCache API key and cache ID have been set for this session.\")\n",
    "else:\n",
    "    print(\"LangCache API key and cache ID found in environment variables.\")\n",
    "\n",
    "\n",
    "if not langcache_api_key or not langcache_cache_id:\n",
    "    print(\"Not running LangCache examples because we do not have an API key and cache ID.\")\n",
    "    exit(0)\n",
    "\n",
    "# Initialize LAngCacheSemanticCache\n",
    "semantic_cache = LangCacheSemanticCache(\n",
    "    cache_id=langcache_cache_id,\n",
    "    api_key=langcache_api_key,\n",
    "    distance_threshold=0.2\n",
    ")\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "\n",
    "# Function to test semantic cache\n",
    "def test_semantic_cache(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# Original query\n",
    "original_prompt = \"What is the capital of France?\"\n",
    "result1, time1 = test_semantic_cache(original_prompt)\n",
    "print(f\"Original query:\\nPrompt: {original_prompt}\\n\")\n",
    "print(f\"Result: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Semantically similar query\n",
    "similar_prompt = \"Can you tell me the capital city of France?\"\n",
    "result2, time2 = test_semantic_cache(similar_prompt)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt}\\n\")\n",
    "print(f\"Result: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the semantic cache\n",
    "semantic_cache.clear()\n",
    "print(\"Semantic cache cleared\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangCache API key and cache ID found in environment variables.\n",
      "17:55:46 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/727a79865ac848c7900caabad5db1afe/entries/search \"HTTP/1.1 200 OK\"\n",
      "17:55:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:47 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/727a79865ac848c7900caabad5db1afe/entries \"HTTP/1.1 201 Created\"\n",
      "Original query:\n",
      "Prompt: What is the capital of France?\n",
      "\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 0.61 seconds\n",
      "\n",
      "17:55:47 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/727a79865ac848c7900caabad5db1afe/entries/search \"HTTP/1.1 200 OK\"\n",
      "17:55:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:47 httpx INFO   HTTP Request: POST https://aws-us-east-1.langcache.redis.io/v1/caches/727a79865ac848c7900caabad5db1afe/entries \"HTTP/1.1 201 Created\"\n",
      "Similar query:\n",
      "Prompt: Can you tell me the capital city of France?\n",
      "\n",
      "Result: \n",
      "\n",
      "The capital city of France is Paris.\n",
      "Time: 0.72 seconds\n",
      "\n",
      "Speed improvement: 0.84x faster\n",
      "17:55:48 httpx INFO   HTTP Request: DELETE https://aws-us-east-1.langcache.redis.io/v1/caches/727a79865ac848c7900caabad5db1afe/entries \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestErrorResponseContent",
     "evalue": "{\"detail\":\"attributes: cannot be blank.\",\"status\":400,\"title\":\"Invalid Request\",\"type\":\"/errors/invalid-data\"}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestErrorResponseContent\u001B[39m            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 59\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSpeed improvement: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime1\u001B[38;5;250m \u001B[39m/\u001B[38;5;250m \u001B[39mtime2\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mx faster\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Clear the semantic cache\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m \u001B[43msemantic_cache\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclear\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSemantic cache cleared\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/src/langchain-redis/libs/redis/langchain_redis/cache.py:858\u001B[39m, in \u001B[36mLangCacheSemanticCache.clear\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    856\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mclear\u001B[39m(\u001B[38;5;28mself\u001B[39m, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    857\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Clear all entries via the wrapper's clear API.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m858\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclear\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/src/langchain-redis/libs/redis/env/lib/python3.11/site-packages/redisvl/extensions/cache/llm/langcache.py:557\u001B[39m, in \u001B[36mLangCacheSemanticCache.clear\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    552\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mclear\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    553\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Clear the cache of all entries.\u001B[39;00m\n\u001B[32m    554\u001B[39m \n\u001B[32m    555\u001B[39m \u001B[33;03m    This is an alias for delete() to match the BaseCache interface.\u001B[39;00m\n\u001B[32m    556\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m557\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdelete\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/src/langchain-redis/libs/redis/env/lib/python3.11/site-packages/redisvl/extensions/cache/llm/langcache.py:542\u001B[39m, in \u001B[36mLangCacheSemanticCache.delete\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    536\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdelete\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    537\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Delete the entire cache.\u001B[39;00m\n\u001B[32m    538\u001B[39m \n\u001B[32m    539\u001B[39m \u001B[33;03m    This deletes all entries in the cache by calling delete_query\u001B[39;00m\n\u001B[32m    540\u001B[39m \u001B[33;03m    with no attributes.\u001B[39;00m\n\u001B[32m    541\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m542\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdelete_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattributes\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/src/langchain-redis/libs/redis/env/lib/python3.11/site-packages/langcache/sdk.py:227\u001B[39m, in \u001B[36mLangCache.delete_query\u001B[39m\u001B[34m(self, attributes, retries, server_url, timeout_ms, http_headers)\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m utils.match_response(http_res, \u001B[33m\"\u001B[39m\u001B[33m400\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mapplication/json\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    224\u001B[39m     response_data = unmarshal_json_response(\n\u001B[32m    225\u001B[39m         errors.BadRequestErrorResponseContentData, http_res\n\u001B[32m    226\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m errors.BadRequestErrorResponseContent(response_data, http_res)\n\u001B[32m    228\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m utils.match_response(http_res, \u001B[33m\"\u001B[39m\u001B[33m401\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mapplication/json\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    229\u001B[39m     response_data = unmarshal_json_response(\n\u001B[32m    230\u001B[39m         errors.AuthenticationErrorResponseContentData, http_res\n\u001B[32m    231\u001B[39m     )\n",
      "\u001B[31mBadRequestErrorResponseContent\u001B[39m: {\"detail\":\"attributes: cannot be blank.\",\"status\":400,\"title\":\"Invalid Request\",\"type\":\"/errors/invalid-data\"}"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Advanced Usage"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom TTL (Time-To-Live)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize RedisCache with custom TTL\n",
    "ttl_cache = RedisCache(redis_url=REDIS_URL, ttl=5)  # 60 seconds TTL\n",
    "\n",
    "# Update a cache entry\n",
    "ttl_cache.update(\"test_prompt\", \"test_llm\", [Generation(text=\"Cached response\")])\n",
    "\n",
    "# Retrieve the cached entry\n",
    "cached_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "print(f\"Cached result: {cached_result[0].text if cached_result else 'Not found'}\")\n",
    "\n",
    "# Wait for TTL to expire\n",
    "print(\"Waiting for TTL to expire...\")\n",
    "time.sleep(6)\n",
    "\n",
    "# Try to retrieve the expired entry\n",
    "expired_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "if expired_result:\n",
    "    print(f\"Result after TTL: {expired_result[0].text}\")\n",
    "else:\n",
    "    print(\"Not found (expired)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Customizing RedisSemanticCache"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize RedisSemanticCache with custom settings\n",
    "custom_semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL,\n",
    "    embeddings=embeddings,\n",
    "    distance_threshold=0.1,  # Stricter similarity threshold\n",
    "    ttl=3600,  # 1 hour TTL\n",
    "    name=\"custom_cache\",  # Custom cache name\n",
    ")\n",
    "\n",
    "# Test the custom semantic cache\n",
    "set_llm_cache(custom_semantic_cache)\n",
    "\n",
    "test_prompt = \"What's the largest planet in our solar system?\"\n",
    "result, _ = test_semantic_cache(test_prompt)\n",
    "print(f\"Original result: {result}\")\n",
    "\n",
    "# Try a slightly different query\n",
    "similar_test_prompt = \"Which planet is the biggest in the solar system?\"\n",
    "similar_result, _ = test_semantic_cache(similar_test_prompt)\n",
    "print(f\"Similar query result: {similar_result}\")\n",
    "\n",
    "# Clean up\n",
    "custom_semantic_cache.clear()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the usage of `RedisCache` and `RedisSemanticCache` from the langchain-redis package. These caching mechanisms can significantly improve the performance of LLM-based applications by reducing redundant API calls and leveraging semantic similarity for intelligent caching. The Redis-based implementation provides a fast, scalable, and flexible solution for caching in distributed systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
