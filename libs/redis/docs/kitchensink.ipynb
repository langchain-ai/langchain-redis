{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Redis Kitchen Sink Example\n",
    "\n",
    "This notebook demonstrates a comprehensive example that combines RedisVectorStore, RedisCache, and RedisChatMessageHistory to create a powerful, efficient, and context-aware chatbot system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-redis langchain-openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-redis version: langchain-redis_v0.0.4\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: T201, I001, E501\n",
    "from langchain_redis.version import __lib_name__\n",
    "\n",
    "print(f\"langchain-redis version: {__lib_name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have a Redis server running. You can start one using Docker with:\n",
    "\n",
    "```\n",
    "docker run -d -p 6379:6379 redis:latest\n",
    "```\n",
    "\n",
    "Or install and run Redis locally according to your operating system's instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://redis:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_redis import RedisVectorStore, RedisCache, RedisChatMessageHistory\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Check if OPENAI_API_KEY is already set in the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API key not found in environment variables.\")\n",
    "    openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "    # Set the API key for the current session\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"OpenAI API key has been set for this session.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Index with RedisVL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll set up our vector store using RedisVL, which provides a powerful interface for creating and managing vector indexes in Redis. We'll define a schema for our Wikipedia data, create an index using RedisVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis import Redis\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema import IndexSchema\n",
    "from langchain_redis import RedisConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RedisVL Index Schema\n",
    "\n",
    "We start by defining a schema for our index. This schema includes:\n",
    "- A text field for the document content\n",
    "- A text field for metadata\n",
    "- A vector field for the document embeddings\n",
    "\n",
    "The vector field is configured with 1536 dimensions (suitable for OpenAI embeddings), using cosine distance and a FLAT index algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\n",
    "            \"name\": \"kitchensink_docs\",\n",
    "            \"storage_type\": \"hash\",\n",
    "            \"prefix\": \"wiki\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"text\", \"type\": \"text\"},\n",
    "            {\"name\": \"url\", \"type\": \"tag\"},\n",
    "            {\"name\": \"title\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 1536,\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                    \"algorithm\": \"FLAT\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RedisVL Index\n",
    "\n",
    "Using the defined schema, we create a SearchIndex object and use it to create the actual index in Redis. This step sets up the structure that our vector store will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish Redis connection and define index\n",
    "redis_client = Redis.from_url(REDIS_URL)\n",
    "\n",
    "# Create the index using RedisVL\n",
    "redisvl_index = SearchIndex(schema, redis_client)\n",
    "redisvl_index.create(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing RedisVectorStore\n",
    "\n",
    "With the RedisVL index in place, we can now initialize our RedisVectorStore. We use a RedisConfig object to specify the index name and Redis URL, ensuring that our vector store connects to the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:27:49 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisVectorStore using the pre-constructed index\n",
    "config = RedisConfig(\n",
    "    index_name=\"kitchensink_docs\", redis_url=REDIS_URL, from_existing=True\n",
    ")\n",
    "vector_store = RedisVectorStore(OpenAIEmbeddings(), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Components\n",
    "\n",
    "We also initialize other components like RedisCache for LLM caching, ChatOpenAI for our language model, and RedisChatMessageHistory for maintaining conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "set_llm_cache(redis_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatOpenAI with caching\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RedisChatMessageHistory\n",
    "message_history = RedisChatMessageHistory(\"kitchensink_chat\", redis_url=REDIS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Vector Store with Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 450 document chunks to the vector store.\n"
     ]
    }
   ],
   "source": [
    "## Populate Vector Store with Wikipedia Data\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def fetch_wikipedia_content(titles):\n",
    "    documents = []\n",
    "    for title in titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            doc = Document(\n",
    "                page_content=page.content, metadata={\"title\": title, \"url\": page.url}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # Choose the first option from the disambiguation list\n",
    "            page = wikipedia.page(e.options[0])\n",
    "            doc = Document(\n",
    "                page_content=page.content,\n",
    "                metadata={\"title\": e.options[0], \"url\": page.url},\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page not found for {title}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Fetch some Wikipedia articles\n",
    "titles = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Deep Learning\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Large Language Models\",\n",
    "    \"Robotics\",\n",
    "]\n",
    "documents = fetch_wikipedia_content(titles)\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_documents(splits)\n",
    "\n",
    "print(f\"Added {len(splits)} document chunks to the vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def combine_chat_history_and_question(inputs):\n",
    "    return f\"Chat History: {inputs['chat_history']}\\nHuman: {inputs['question']}\"\n",
    "\n",
    "\n",
    "# Update the prompt template to include chat history\n",
    "prompt_template = \"\"\"\n",
    "    You are an AI assistant answering questions based on the provided context and chat history. Be concise and accurate.\n",
    "\n",
    "    Context: {context}\n",
    "    {question}\n",
    "    AI Assistant:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        \"question\": combine_chat_history_and_question,\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Assistant! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  What are the principal tenets of A.I.?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "The principal tenets of AI include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. These goals are achieved through the use of various techniques such as search and optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon fields such as psychology, linguistics, philosophy, neuroscience, and others. The ultimate goal of AI research is to develop artificial general intelligence, which would be able to solve a wide variety of problems with the same breadth and versatility as human intelligence.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  What is the relationship between AI and robotics?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "AI and robotics have a close relationship, as AI is often used to control and guide robots in their actions and decision-making processes. Robotics also relies on AI for tasks such as perception, learning, and planning. However, AI and robotics are not synonymous, as AI can exist and be used in other fields and applications outside of robotics.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the AI Assistant!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "The core tenets of AI include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. These are the traditional goals of AI research and are essential for the development and use of AI technology. Additionally, ethical considerations and the promotion of the wellbeing of individuals and communities are also important factors in the development and implementation of AI systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  How does AI influence Robotics and vice versa?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "AI and robotics have a symbiotic relationship, with advancements in one field often leading to advancements in the other. AI technology is used to enhance the capabilities of robots, allowing them to interact with their environment and make decisions based on their programming. On the other hand, robotics provides a physical platform for AI systems to operate in the real world. As AI continues to evolve, it will likely have a significant impact on the development and use of robotics, and vice versa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the AI Assistant!\n"
     ]
    }
   ],
   "source": [
    "def get_chat_history(history):\n",
    "    return \"\\n\".join(\n",
    "        [f\"{msg.type.capitalize()}: {msg.content}\" for msg in history.messages[-5:]]\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Welcome to the AI Assistant! Type 'exit' to end the conversation.\")\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"Human: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user message to history\n",
    "    message_history.add_user_message(user_input)\n",
    "\n",
    "    # Get response from RAG chain\n",
    "    result = rag_chain.invoke(\n",
    "        {\"question\": user_input, \"chat_history\": get_chat_history(message_history)}\n",
    "    )\n",
    "\n",
    "    # Add AI message to history\n",
    "    message_history.add_ai_message(result)\n",
    "\n",
    "    print(f\"AI: {result}\")\n",
    "\n",
    "print(\"Thank you for using the AI Assistant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Kitchen Sink Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the integration of multiple Redis-based components in LangChain:\n",
    "\n",
    "1. **RedisVectorStore**: Used to store and retrieve document chunks from Wikipedia articles. It enables efficient similarity search for relevant context.\n",
    "\n",
    "2. **RedisCache**: Implemented to cache LLM responses, potentially speeding up repeated or similar queries.\n",
    "\n",
    "3. **RedisChatMessageHistory**: Stores the conversation history, allowing the AI to maintain context across multiple interactions.\n",
    "\n",
    "The combination of these components creates a powerful, context-aware chatbot system with the following features:\n",
    "\n",
    "- **Efficient Information Retrieval**: The vector store allows quick access to relevant information from a large dataset.\n",
    "- **Improved Response Time**: Caching helps in reducing API calls for similar or repeated queries.\n",
    "- **Contextual Understanding**: The chat history enables the AI to reference previous parts of the conversation.\n",
    "- **Scalability**: Redis as a backend allows this system to handle large amounts of data and high traffic efficiently.\n",
    "\n",
    "This kitchen sink example showcases how these Redis-based components can work together seamlessly in a real-world application, demonstrating the power and flexibility of the langchain-redis package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "# Clear vector store\n",
    "vector_store.index.delete(drop=True)\n",
    "\n",
    "# Clear cache\n",
    "redis_cache.clear()\n",
    "\n",
    "# Clear chat history\n",
    "message_history.clear()\n",
    "\n",
    "print(\"Cleanup completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
